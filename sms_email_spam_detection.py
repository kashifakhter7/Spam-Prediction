# -*- coding: utf-8 -*-
"""SMS_Email Spam Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A2GOGUz1SkzOSNbPWGFQUCr_R5HDGvy4
"""

import numpy as np
import pandas as pd

#Data set: SMS Spam Collection Dataset, UCI Machine Learning
df = pd.read_csv('mail_data.csv', encoding='latin-1')

df.head(5)

df.shape

'''
Flow of Project:
01: Data Cleaning
02: EDA
03: Text Preprocessing
04: Model Building
05: Evaluation
06: Improvement
07: Website
08: Deployment
'''

"""#01: DATA CLEANING"""

df.info()

df.head()

#drop last 3 column
#df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)

df.sample(5)

#rename
df.rename(columns={'Category':'target', 'Message':'text'}, inplace=True)

df.sample(5)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['target'] = encoder.fit_transform(df['target'])

df.sample(5)

#missing value
df.isnull().sum()

#duplicate value
df.duplicated().sum()

#remove duplicates
df = df.drop_duplicates(keep='first')

df.shape

"""#02: EDA"""

df.head()

df['target'].value_counts()

import matplotlib.pyplot as plt
plt.pie(df['target'].value_counts(), labels=['ham', 'spam'], autopct="%0.2f")
plt.show()

#Data is imbalance

import nltk

nltk.download('punkt')

nltk.download('punkt_tab')

df['text'].apply(len)

df['num_characters'] = df['text'].apply(len)

#num of word
df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))

#no. sentences
df['num_sentences'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.head()

df[['num_characters', 'num_words', 'num_sentences']].describe()

#ham
df[df['target']== 0][['num_characters', 'num_words', 'num_sentences']].describe()

#spam
df[df['target']== 1][['num_characters', 'num_words', 'num_sentences']].describe()

"""#03: Data Preprocessing
1. Lower Case
2. Tokenizatio
3. Removing special character
4. Removing Stop words and punctuation
5. Stemming (Dace = damcing, dances)



"""

import nltk
import string
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
nltk.download('stopwords')

def transform_text(text):
  text = text.lower()
  text = nltk.word_tokenize(text)
  y = []
  for i in text:
    if i.isalnum():
      y.append(i)

  text = y[:]
  y.clear()

  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)

  text = y[:]
  y.clear()

  for i in text:
    y.append(ps.stem(i))
  return " ".join(y)

df['transformed_text']= df['text'].apply(transform_text)

df.head()

from wordcloud import WordCloud
wc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')

spamwc= wc.generate(df[df['target']==1]['transformed_text'].str.cat(sep=" "))

plt.imshow(spamwc)

hamwc= wc.generate(df[df['target']==0]['transformed_text'].str.cat(sep=" "))

plt.imshow(hamwc)

"""#04: Model Building"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
tfidf = TfidfVectorizer()

x =tfidf.fit_transform(df['transformed_text']).toarray()

x.shape

y = df['target'].values

y

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score

gnb = GaussianNB()
mnb= MultinomialNB()
bnb= BernoulliNB()

gnb.fit(X_train,y_train)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(X_train,y_train)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))

bnb.fit(X_train,y_train)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))

import pickle
pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(mnb,open('model.pkl','wb'))

user_input = input("Enter a message to classify as spam or ham: ")
processed_input = transform_text(user_input)
vectorized_input = tfidf.transform([processed_input])
prediction = mnb.predict(vectorized_input)

if prediction[0] == 1:
  print("This is a spam message.")
else:
  print("This is a ham message.")

